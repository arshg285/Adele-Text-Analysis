---
title: "36-668: Coffee Break Experiment 1"
author: "Arsh Gupta"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Loading in libraries.

```{r message = FALSE}
library(readtext)
library(quanteda)
library(cmu.textstat)
library(quanteda.textstats)
library(dplyr)
library(tidyr)
library(knitr)
library(ggraph)
library(igraph)
```

Use `readtext()` function from `readtext` package to create a `data.frame` object.

```{r}
rt1 <- readtext("album_19_1/*.txt",
                docvarsfrom = "filenames",
                docvarnames = c("type", "album", "song_rank"))

rt2 <- readtext("album_21_2/*.txt",
                docvarsfrom = "filenames",
                docvarnames = c("type", "album", "song_rank"))

rt3 <- readtext("album_25_3/*.txt",
                docvarsfrom = "filenames",
                docvarnames = c("type", "album", "song_rank"))

rt4 <- readtext("album_30_4/*.txt",
                docvarsfrom = "filenames",
                docvarnames = c("type", "album", "song_rank"))
```

Make a `corpus` object.

```{r}
album1_corpus <- corpus(rt1)
album2_corpus <- corpus(rt2)
album3_corpus <- corpus(rt3)
album4_corpus <- corpus(rt4)

rt_12 <- rbind(rt1, rt2)
rt_34 <- rbind(rt3, rt4)
rt_all <- rbind(rt1, rt2, rt3, rt4)

albums_12_corpus <- corpus(rt_12)
albums_34_corpus <- corpus(rt_34)
all_albums_corpus <- corpus(rt_all)
```

Check the result.

```{r}
knitr::kable(head(album1_corpus %>% summary()), caption = "Partial summary of album1 corpus.")

knitr::kable(head(album2_corpus %>% summary()), caption = "Partial summary of album2 corpus.")

knitr::kable(head(album3_corpus %>% summary()), caption = "Partial summary of album3 corpus.")

knitr::kable(head(album4_corpus %>% summary()), caption = "Partial summary of album4 corpus.")
```

We'll use **quanteda** to tokenize. And after tokenization, we'll convert them to lower case. As a next step, we'll being combining tokens like *a* and *lot* into single units. And we'll be using a list of expressions that isn't case sensitive.

```{r}
album1_tokens <- tokens(album1_corpus,
                        include_docvars = TRUE,
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE,
                        what = "word")

album1_tokens <- tokens_tolower(album1_tokens)

album2_tokens <- tokens(album2_corpus,
                        include_docvars = TRUE,
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE,
                        what = "word")

album2_tokens <- tokens_tolower(album2_tokens)

album3_tokens <- tokens(album3_corpus,
                        include_docvars = TRUE,
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE,
                        what = "word")

album3_tokens <- tokens_tolower(album3_tokens)

album4_tokens <- tokens(album4_corpus,
                        include_docvars = TRUE,
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE, what = "word")

album4_tokens <- tokens_tolower(album4_tokens)

all_albums_tokens <- tokens(all_albums_corpus,
                        include_docvars=TRUE,
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE,
                        what = "word")

albums_12_tokens <- tokens(albums_12_corpus,
                        include_docvars=TRUE,
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE,
                        what = "word")

albums_34_tokens <- tokens(albums_34_corpus,
                        include_docvars=TRUE,
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE,
                        what = "word")
```

An issue that we run into frequently with corpus analysis is what to do with multi-word expressions. For example, consider a common English quantifier: "a lot". Typical tokenization rules will split this into two tokens: *a* and *lot*. But counting *a lot* as a single unit might be important depending on our task. We have a way of telling **quanteda** to account for these tokens.

All that we need is a list of multi-word expressions.

The **cmu.textstat** comes with an example of an mwe list called **multiword_expressions**:

```{r}
album1_tokens <- tokens_compound(album1_tokens,
                                 pattern = phrase(multiword_expressions))

album2_tokens <- tokens_compound(album2_tokens,
                                 pattern = phrase(multiword_expressions))

album3_tokens <- tokens_compound(album3_tokens,
                                 pattern = phrase(multiword_expressions))

album4_tokens <- tokens_compound(album4_tokens,
                                 pattern = phrase(multiword_expressions))

all_albums_tokens <- tokens_compound(all_albums_tokens,
                                     pattern = phrase(multiword_expressions))

albums_12_tokens <- tokens_compound(albums_12_tokens,
                                    pattern = phrase(multiword_expressions))

albums_34_tokens <- tokens_compound(albums_34_tokens,
                                    pattern = phrase(multiword_expressions))
```

With our tokens object we can now create a document-feature-matrix using the
**dfm()** function. As a reminder, a **dfm** is table with one row per document in the corpus, and one column per unique token in the corpus. Each cell contains a count of how many times a token shows up in that document.

```{r}
album1_dfm <- dfm(album1_tokens)
album2_dfm <- dfm(album2_tokens)
album3_dfm <- dfm(album3_tokens)
album4_dfm <- dfm(album4_tokens)

all_albums_dfm <- dfm(all_albums_tokens)
albums_12_dfm <- dfm(albums_12_tokens)
albums_34_dfm <- dfm(albums_34_tokens)
```

Next we'll create a **dfm** with proportionally weighted counts. We will create another corpus for all the albums.

```{r}
prop_album1_dfm <- dfm_weight(album1_dfm, scheme = "prop")
prop_album2_dfm <- dfm_weight(album2_dfm, scheme = "prop")
prop_album3_dfm <- dfm_weight(album3_dfm, scheme = "prop")
prop_album4_dfm <- dfm_weight(album4_dfm, scheme = "prop")

prop_all_albums_dfm <- dfm_weight(all_albums_dfm, scheme = "prop")
prop_albums_12_dfm <- dfm_weight(albums_12_dfm, scheme = "prop")
prop_albums_34_dfm <- dfm_weight(albums_34_dfm, scheme = "prop")
```

Use textstat_frequency to calculate the frequencies for the entire 4 albums, for albums 1 and 2, and for albums 3 and 4.

```{r}
freq_df <- textstat_frequency(all_albums_dfm) %>%
  data.frame(stringsAsFactors = F) %>%
  select(feature, frequency) %>%
  rename("Token" = "feature", "Frequency" = "frequency")

kable(head(freq_df))

freq_df_12 <- textstat_frequency(albums_12_dfm) %>%
  data.frame(stringsAsFactors = F) %>%
  select(feature, frequency) %>%
  rename("Token" = "feature", "Frequency" = "frequency")

kable(head(freq_df_12))

freq_df_34 <- textstat_frequency(albums_34_dfm) %>%
  data.frame(stringsAsFactors = F) %>%
  select(feature, frequency) %>%
  rename("Token" = "feature", "Frequency" = "frequency")

kable(head(freq_df_34))
```

Now, we calculate dispersion tokens for our two dfm's

```{r}
dispersion_all_albums <- all_albums_dfm %>% dispersions_all()
dispersion_albums_12 <- albums_12_dfm %>% dispersions_all()
dispersion_albums_34 <- albums_34_dfm %>% dispersions_all()

kable(head(dispersion_all_albums))
kable(head(dispersion_albums_12))
kable(head(dispersion_albums_34))
```

Now we create collocates!

```{r}
albums_12_love_collocates <- collocates_by_MI(albums_12_tokens, "love") %>%
  filter(col_freq >= 4 & MI_1 >= 4)

albums_12_heart_collocates <- collocates_by_MI(albums_12_tokens, "heart") %>%
  filter(col_freq >= 4 & MI_1 >= 4)

albums_34_love_collocates <- collocates_by_MI(albums_34_tokens, "love") %>%
  filter(col_freq >= 4 & MI_1 >= 4)

albums_34_heart_collocates <- collocates_by_MI(albums_34_tokens, "heart") %>%
  filter(col_freq >= 4 & MI_1 >= 4)
```

Now, we create a graph for `love`.

```{r}
net <- col_network(albums_12_love_collocates, albums_34_love_collocates)

ggraph(net, layout = "stress") +
  
  geom_edge_link(color = "grey80", alpha = 0.75) +
  
  geom_node_point(aes(alpha = node_weight, size = 3, color = n_intersects)) +
  
  geom_node_text(aes(label = label), repel = T, size = 3) +
  
  scale_alpha(range = c(0.2, 0.9)) +
  
  theme_graph() +
  
  theme(legend.position = "none")
```

Now, we create a graph for `heart`.

```{r}
net <- col_network(albums_12_heart_collocates, albums_34_heart_collocates)

ggraph(net, layout = "stress") +
  
  geom_edge_link(color = "grey80", alpha = 0.75) +
  
  geom_node_point(aes(alpha = node_weight, size = 3, color = n_intersects)) +
  
  geom_node_text(aes(label = label), repel = T, size = 3) +
  
  scale_alpha(range = c(0.2, 0.9)) +
  
  theme_graph() +
  
  theme(legend.position = "none")
```